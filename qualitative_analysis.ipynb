{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions = pd.read_csv('analysis-data/all_questions.csv')\n",
    "df_pairs_correct = pd.read_csv('analysis-data/large_correct_sample_with_pred.csv')\n",
    "df_pairs_incorrect = pd.read_csv('analysis-data/large_incorrect_sample_with_pred.csv')\n",
    "df_both = pd.concat([df_pairs_correct, df_pairs_incorrect])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean belief change prediction given correct: 0.324\n",
      "Mean belief change prediction given incorrect: 0.455\n"
     ]
    }
   ],
   "source": [
    "belief_change_pred_given_correct_mean = df_pairs_correct['pred'].mean()\n",
    "belief_change_pred_given_incorrect_mean = df_pairs_incorrect['pred'].mean()\n",
    "print(f\"Mean belief change prediction given correct: {belief_change_pred_given_correct_mean:.3f}\")\n",
    "print(f\"Mean belief change prediction given incorrect: {belief_change_pred_given_incorrect_mean:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get extreme predictions\n",
    "likely_to_change_correct = df_pairs_correct[df_pairs_correct['pred'] >= np.percentile(df_pairs_correct['pred'], 90)]\n",
    "likely_to_change_incorrect = df_pairs_incorrect[df_pairs_incorrect['pred'] >= np.percentile(df_pairs_incorrect['pred'], 90)]\n",
    "unlikely_to_change_correct = df_pairs_correct[df_pairs_correct['pred'] <= np.percentile(df_pairs_correct['pred'], 10)]\n",
    "unlikely_to_change_incorrect = df_pairs_incorrect[df_pairs_incorrect['pred'] <= np.percentile(df_pairs_incorrect['pred'], 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5134</th>\n",
       "      <td>&lt;b&gt;Alice, Bob, Claire, Dave, and Eve are holdi...</td>\n",
       "      <td>&lt;b&gt;Alice, Bob, Claire, Dave, and Eve are playi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>&lt;b&gt;Alice, Bob, and Claire are holding a white ...</td>\n",
       "      <td>&lt;b&gt;Alice, Bob, Claire, Dave, and Eve are playi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5360</th>\n",
       "      <td>&lt;b&gt;Alice, Bob, Claire, Dave, and Eve are holdi...</td>\n",
       "      <td>&lt;b&gt;Alice, Bob, and Claire are holding a white ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     q1  \\\n",
       "5134  <b>Alice, Bob, Claire, Dave, and Eve are holdi...   \n",
       "2011  <b>Alice, Bob, and Claire are holding a white ...   \n",
       "5360  <b>Alice, Bob, Claire, Dave, and Eve are holdi...   \n",
       "\n",
       "                                                     q2  \n",
       "5134  <b>Alice, Bob, Claire, Dave, and Eve are playi...  \n",
       "2011  <b>Alice, Bob, Claire, Dave, and Eve are playi...  \n",
       "5360  <b>Alice, Bob, and Claire are holding a white ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get top few questions where beliefs are likely to change (given an incorrect response)\n",
    "likely_to_change_incorrect.sort_values(by='pred', ascending=False).head(3)[['q1', 'q2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1</th>\n",
       "      <th>q1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>&lt;b&gt;For which of these two scenarios does the m...</td>\n",
       "      <td>&lt;b&gt;For which of these two scenarios does the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8463</th>\n",
       "      <td>&lt;b&gt;For which of these two scenarios does the m...</td>\n",
       "      <td>&lt;b&gt;For which of these two scenarios does the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3673</th>\n",
       "      <td>&lt;b&gt;For which of these two scenarios does the m...</td>\n",
       "      <td>&lt;b&gt;For which of these two scenarios does the m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     q1  \\\n",
       "3749  <b>For which of these two scenarios does the m...   \n",
       "8463  <b>For which of these two scenarios does the m...   \n",
       "3673  <b>For which of these two scenarios does the m...   \n",
       "\n",
       "                                                     q1  \n",
       "3749  <b>For which of these two scenarios does the m...  \n",
       "8463  <b>For which of these two scenarios does the m...  \n",
       "3673  <b>For which of these two scenarios does the m...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get top few questions where beliefs are likely to change (given a correct response)\n",
    "likely_to_change_correct.sort_values(by='pred', ascending=False).head(3)[['q1', 'q1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>&lt;b&gt;What type of covalent bonds link the amino ...</td>\n",
       "      <td>&lt;b&gt;Glycogen breakdown in muscle initially resu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    q1  \\\n",
       "889  <b>What type of covalent bonds link the amino ...   \n",
       "\n",
       "                                                    q2  \n",
       "889  <b>Glycogen breakdown in muscle initially resu...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get qualitative examples\n",
    "likely_to_change_incorrect.loc[[889]][['q1', 'q2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9564</th>\n",
       "      <td>&lt;b&gt;Which of the following articles are not qua...</td>\n",
       "      <td>&lt;b&gt;Is there any priority among international c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     q1  \\\n",
       "9564  <b>Which of the following articles are not qua...   \n",
       "\n",
       "                                                     q2  \n",
       "9564  <b>Is there any priority among international c...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likely_to_change_correct.loc[[9564]][['q1', 'q2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>&lt;b&gt;Which of the boys on the TV show 'My Three ...</td>\n",
       "      <td>((-6 - 4 * 2 - 6) + (1 + -2 * 1 * 7)) =</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    q1  \\\n",
       "417  <b>Which of the boys on the TV show 'My Three ...   \n",
       "\n",
       "                                          q2  \n",
       "417  ((-6 - 4 * 2 - 6) + (1 + -2 * 1 * 7)) =  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlikely_to_change_correct.loc[[417]][['q1', 'q2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;b&gt;On the nightstand, there is a fuchsia jug, ...</td>\n",
       "      <td>&lt;b&gt;Which of the following vitamins provides th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   q1  \\\n",
       "14  <b>On the nightstand, there is a fuchsia jug, ...   \n",
       "\n",
       "                                                   q2  \n",
       "14  <b>Which of the following vitamins provides th...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find pairs where correct versus incorrect makes a large difference\n",
    "merged_df = df_pairs_correct.merge(df_pairs_incorrect, on=['q1', 'q2', 'source1', 'source2'], suffixes=('_correct', '_incorrect'))\n",
    "merged_df = merged_df[['q1', 'q2', 'source1', 'source2', 'pred_correct', 'pred_incorrect']]\n",
    "diffs = merged_df['pred_correct'] - merged_df['pred_incorrect']\n",
    "merged_df['diff'] = diffs\n",
    "diffs_important_df = merged_df[merged_df['diff'] < np.percentile(merged_df['diff'], 6.4)]\n",
    "diffs_important_df.loc[[14]][['q1', 'q2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find examples where LLM misalignment leads to human generalization failures.\n",
    "# Load survey data:\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "df_full = pd.concat([df_train, df_test])\n",
    "# Find examples where previous correct responses led to belief change:\n",
    "df_belief_change = df_full[(df_full['belief_change'] == 1) & (df_full['previous_correct'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5321</th>\n",
       "      <td>&lt;b&gt;Suppose that current disposable income is $...</td>\n",
       "      <td>((-8 * 1 * -1 + -5) + (-2 - -3 + 1 * -7)) =</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3285</th>\n",
       "      <td>&lt;b&gt;Alice, Bob, and Claire are on the same team...</td>\n",
       "      <td>&lt;b&gt;Alice, Bob, and Claire are on the same team...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11245</th>\n",
       "      <td>&lt;b&gt;According to Hume, morality is ultimately b...</td>\n",
       "      <td>&lt;b&gt;For which of these two scenarios does the m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      q1  \\\n",
       "5321   <b>Suppose that current disposable income is $...   \n",
       "3285   <b>Alice, Bob, and Claire are on the same team...   \n",
       "11245  <b>According to Hume, morality is ultimately b...   \n",
       "\n",
       "                                                      q2  \n",
       "5321         ((-8 * 1 * -1 + -5) + (-2 - -3 + 1 * -7)) =  \n",
       "3285   <b>Alice, Bob, and Claire are on the same team...  \n",
       "11245  <b>For which of these two scenarios does the m...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inds = [5321, 3285, 11245]\n",
    "sample = df_belief_change.loc[inds][['q1', 'q2']]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import together\n",
    "import utils\n",
    "# For querying LLM.\n",
    "# NOTE: Replace API key here with your own.\n",
    "together.api_key = \"XXX\"\n",
    "\n",
    "# This code verifies whether an LLM gets the first one correct while responding to the second incorrectly.\n",
    "\n",
    "def get_correct(llm, previous_q, current_q, previous_subject, current_subject):\n",
    "  previous_target = all_questions.loc[all_questions['question'] == previous_q]['answer'].values[0]\n",
    "  current_target = all_questions.loc[all_questions['question'] == current_q]['answer'].values[0]\n",
    "  previous_task = utils.get_task(previous_subject)\n",
    "  previous_response_type = utils.get_response_type(previous_task)\n",
    "  previous_valid_responses, previous_prompt_end = utils.get_valid_responses(previous_response_type, previous_task)\n",
    "  previous_prompt = previous_q.replace(\"<br>\", \"\\n\").replace('<b>', '').replace('</b>', '') + \"\\n\" + previous_prompt_end\n",
    "  previous_correct = utils.check_answer(utils.get_answer(previous_prompt, llm), previous_target, previous_response_type, previous_valid_responses)\n",
    "  #\n",
    "  current_task = utils.get_task(current_subject)\n",
    "  current_response_type = utils.get_response_type(current_task)\n",
    "  current_valid_responses, current_prompt_end = utils.get_valid_responses(current_response_type, current_task)\n",
    "  current_prompt = current_q.replace(\"<br>\", \"\\n\").replace('<b>', '').replace('</b>', '') + \"\\n\" + current_prompt_end\n",
    "  current_correct = utils.check_answer(utils.get_answer(current_prompt, llm), current_target, current_response_type, current_valid_responses)\n",
    "  return previous_correct, current_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C C 1\n",
      "14 -3 0\n",
      "C C 1\n",
      "C A 0\n",
      "D D 1\n",
      "C B 0\n"
     ]
    }
   ],
   "source": [
    "llm = 'llama-2-70b-chat'\n",
    "prev_corrects = []\n",
    "current_corrects = []\n",
    "for i, row in sample.iterrows():\n",
    "  prev_correct, current_correct = get_correct(llm, row['q1'], row['q2'], row['source1'], row['source2'])\n",
    "  prev_corrects.append(prev_correct)\n",
    "  current_corrects.append(current_correct)\n",
    "sample['llm_prev_correct'] = prev_corrects\n",
    "sample['llm_current_correct'] = current_corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['<b>Suppose that current disposable income is $10000 and consumption spending is $8000. For every $100 increase in disposable income saving increases $10. Given this information<br></b>(A) the marginal propensity to consume is .80.<br>(B) the marginal propensity to save is .20.<br>(C) the marginal propensity to save is .10.<br>(D) the marginal propensity to save is .90.',\n",
       "        '((-8 * 1 * -1 + -5) + (-2 - -3 + 1 * -7)) =',\n",
       "        'mmlu_high_school_macroeconomics',\n",
       "        'bbh_multistep_arithmetic_two', 1, 1, 1, 0],\n",
       "       ['<b>Alice, Bob, and Claire are on the same team in a soccer match. At the start of the match, they are each assigned to a position: Alice is playing fullback, Bob is playing cheerleader, and Claire is playing left winger.<br>As the game progresses, pairs of players occasionally swap positions. First, Alice and Claire trade positions. Then, Bob and Alice trade positions. Finally, Bob and Claire trade positions. At the end of the match, Claire is playing<br>Options:<br></b>(A) fullback<br>(B) cheerleader<br>(C) left winger',\n",
       "        '<b>Alice, Bob, and Claire are on the same team in a soccer match. At the start of the match, they are each assigned to a position: Alice is playing right winger, Bob is playing cheerleader, and Claire is playing striker.<br>As the game progresses, pairs of players occasionally swap positions. First, Alice and Claire trade positions. Then, Claire and Bob trade positions. Finally, Bob and Alice trade positions. At the end of the match, Alice is playing<br>Options:<br></b>(A) right winger<br>(B) cheerleader<br>(C) striker',\n",
       "        'bbh_tracking_shuffled_objects_three_objects',\n",
       "        'bbh_tracking_shuffled_objects_three_objects', 1, 1, 1, 0],\n",
       "       ['<b>According to Hume, morality is ultimately based on:<br></b>(A) reason.<br>(B) Godâ€™s commands.<br>(C) social agreements.<br>(D) sympathy.',\n",
       "        '<b>For which of these two scenarios does the main character (who uses I/me/my) do something clearly morally wrong, according to ordinary moral standards in the US as of 2020? Scenario 1 | I stuck a knife in his black heart. Scenario 2 | I took games away from my son as a punishment for skipping school.<br></b>(A) Wrong, Wrong<br>(B) Wrong, Not wrong<br>(C) Not wrong, Wrong<br>(D) Not wrong, Not wrong',\n",
       "        'mmlu_philosophy', 'mmlu_moral_scenarios', 1, 1, 1, 0]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[(sample['llm_prev_correct'] == 1) & \n",
    "       (sample['llm_current_correct'] == 0) & \n",
    "       (sample['previous_correct'] == 1)].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
